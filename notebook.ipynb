{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char-RNN Text Generation\n",
    "\n",
    "The following recurrent neural network generated Shakespeare text. We feed in text from `data/tiny-shakespeare.txt` into the network character by character and at each time step we want to predict the *next* character (see imag below). After training our network on Shakespearean data we proceed to sample from it by providing with a random character and then letting it predict the next. We then feed this next character back into the network and let it predict the next character, and so on. With luck, our network will output an award-winning play! \n",
    "\n",
    "To run the below code just select the cell it is written in and press `Shift + Enter`.\n",
    "\n",
    "![](rnn-data/char-rnn.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 1115393 characters, 65 unique.\n",
      "EPOCH:\t0\n",
      "iter 1, loss: 104.2594703945225\n",
      "iter 1000, loss: 40.45898636330465\n",
      "iter 2000, loss: 16.804994173761436\n",
      "iter 3000, loss: 7.898309867180023\n",
      "iter 4000, loss: 4.50954681471479\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Load data\n",
    "data = open('data/tiny-shakespeare.txt', 'r').read() \n",
    "chars = list(set(data))\n",
    "data_size, chars_size = len(data), len(chars)\n",
    "\n",
    "# Print info\n",
    "print('Data has {} characters, {} unique.'.format(data_size, chars_size))\n",
    "\n",
    "# Conversion dicts\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# A method to create a trainset X: inputs, Y: targets\n",
    "def create_trainset(text, seq_length):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    # Loop over data and create x,y training pairs\n",
    "    for i in range(int(len(data)/seq_length)):\n",
    "            x = [char_to_ix[c] for c in data[i*seq_length:(i+1)*seq_length]] #inputs to the RNN\n",
    "            y = [char_to_ix[c] for c in data[i*seq_length+1:(i+1)*seq_length+1]] #the targets it should be outputting\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "            \n",
    "    return X, Y\n",
    "\n",
    "###########################\n",
    "# RNN MODEL\n",
    "###########################\n",
    "\n",
    "# Example and label vecs\n",
    "x = T.fmatrix('x')\n",
    "y = T.ivector('y')\n",
    "\n",
    "# Data info\n",
    "seq_length = 25\n",
    "indim = chars_size\n",
    "hdim = 100\n",
    "outdim = chars_size\n",
    "X, Y = create_trainset(data, seq_length)\n",
    "data_size = len(X)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "grad_clip = [-5, 5]\n",
    "L1_reg = 0.0\n",
    "L2_reg = 0.0\n",
    "\n",
    "# Parameters\n",
    "# np.random.seed(256)\n",
    "W_in = theano.shared(name='w_in', value=np.random.random((hdim,indim)).astype(theano.config.floatX)*0.01)\n",
    "W_h = theano.shared(name='w_h', value=np.random.random((hdim,hdim)).astype(theano.config.floatX)*0.01)\n",
    "W_out = theano.shared(name='w_out', value=np.random.random((outdim,hdim)).astype(theano.config.floatX)*0.01)\n",
    "b_h = theano.shared(name='b_h', value=np.random.random((hdim,)).astype(theano.config.floatX))\n",
    "b_out = theano.shared(name='b_out', value=np.random.random((outdim,)).astype(theano.config.floatX))\n",
    "h_0 = theano.shared(name='h_0', value=np.zeros((hdim,)).astype(theano.config.floatX))\n",
    "\n",
    "params = [W_in, W_h, W_out, b_h, b_out, h_0]\n",
    "\n",
    "def step(x_t, h_tm1):\n",
    "    h_t = T.tanh(T.dot(W_in, x_t) + T.dot(W_h, h_tm1) + b_h)\n",
    "    out_t = T.nnet.softmax(T.dot(W_out, h_t) + b_out)\n",
    "    return [h_t, out_t]\n",
    "\n",
    "[h, out], _ = theano.scan(fn=step,\n",
    "                          sequences=x,\n",
    "                          outputs_info=[h_0, None],\n",
    "                          n_steps=seq_length)\n",
    "\n",
    "# Output \n",
    "p_y_given_x = out[:, 0, :]\n",
    "y_pred = T.argmax(p_y_given_x, axis=1)\n",
    "\n",
    "# Regularization\n",
    "L1 = L1_reg * (abs(W_in.sum()) + abs(W_h.sum()) + abs(W_out.sum()))\n",
    "L2 = L2_reg * (abs(W_in.sum() ** 2) + abs(W_h.sum() ** 2) + abs(W_out.sum() ** 2))\n",
    "\n",
    "# Loss\n",
    "loss = -T.mean(T.log(p_y_given_x)[T.arange(x.shape[0]), y] + L1 + L2) \n",
    "\n",
    "# Gradient (the derivative of the loss w.r.t. the params)\n",
    "dLossdParams = T.grad(loss, params)\n",
    "dLossdParams_clipped = [T.clip(g, grad_clip[0], grad_clip[1]) for g in dLossdParams]\n",
    "gradient_updates = OrderedDict((p, p - learning_rate*g) for p, g in zip(params, dLossdParams_clipped))\n",
    "\n",
    "# Output functions\n",
    "train_step = theano.function(inputs=[x, y], outputs=loss, updates=gradient_updates)\n",
    "get_hprev = theano.function(inputs=[x], outputs=h[-1])\n",
    "\n",
    "###########################\n",
    "# SAMPLING CODE\n",
    "###########################\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((chars_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(W_in.get_value(), x) + np.dot(W_h.get_value(), h) + np.reshape(b_h.get_value(), (-1, 1)))\n",
    "    y = np.dot(W_out.get_value(), h) + np.reshape(b_out.get_value(), (-1,1))\n",
    "    p = np.exp(y) / np.sum(np.exp(y)) # softmax\n",
    "    ix = np.random.choice(range(chars_size), p=p.ravel())\n",
    "    x = np.zeros((chars_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "###########################\n",
    "# TRAINING CODE\n",
    "###########################\n",
    "\n",
    "smooth_loss = -np.log(1.0/chars_size)*seq_length # loss at iteration 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('EPOCH:\\t{}'.format(epoch))\n",
    "    loss = 0\n",
    "    i = 1\n",
    "    for x,y in zip(X,Y):\n",
    "        \n",
    "        # Convert x (a list of indices) into a matrix with one 1-hot example vector per row\n",
    "        x_vecs = []\n",
    "        for ix in x:\n",
    "            vec = np.zeros(chars_size)\n",
    "            vec[ix] = 1\n",
    "            x_vecs.append(vec)\n",
    "        x_mat = np.array(x_vecs, dtype='float32')\n",
    "        \n",
    "        # Convert y into a vector\n",
    "        y_vec = np.array(y, dtype='int32')\n",
    "        \n",
    "        # Use x_mat and y_vec to train the RNN\n",
    "        loss = train_step(x_mat, y_vec)\n",
    "        \n",
    "        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "        \n",
    "        # Print progress\n",
    "        if i == 1 or (i % 1000 == 0) or i == data_size+1:\n",
    "            print('iter {}, loss: {}'.format(i, smooth_loss))\n",
    "            hprev = np.reshape(get_hprev(x_mat), (-1,1)) # turn this into a 2d column vector\n",
    "            ixes = sample(hprev,x[0],200)\n",
    "            \n",
    "            # Sample every few iterations\n",
    "            if(i % 5000 == 0):\n",
    "                print(\"\\n\")\n",
    "                for ix in ixes:\n",
    "                    print(ix_to_char[ix], end=\"\")\n",
    "                print(\"\\n\")\n",
    "                \n",
    "        # Update counter\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py34]",
   "language": "python",
   "name": "Python [py34]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
